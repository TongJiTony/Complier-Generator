# Auto-generated by generate_lexer.py
import re

token_specs = [
    ("IF", r"if"),
    ("ELSE", r"else"),
    ("WHILE", r"while"),
    ("ID", r"[a-zA-Z_][a-zA-Z0-9_]*"),
    ("NUM", r"[0-9]+"),
    ("ASSIGN", r"="),
    ("PLUS", r"\+"),
    ("MINUS", r"-"),
    ("MUL", r"\*"),
    ("DIV", r"/"),
    ("LPAREN", r"\("),
    ("RPAREN", r"\)"),
    ("SEMI", r";"),
    ("RELOP", r"(==|!=|<=|>=|<|>)"),
    ("AND", r"&&"),
    ("OR", r"\|\|"),
    ("NOT", r"!"),
    ("WS", r"[ \t\n]+"),
]
token_regex = "|".join(f"(?P<{t}>{p})" for t,p in token_specs)
master_re = re.compile(token_regex)

def tokenize(text):
    pos = 0
    tokens = []
    while pos < len(text):
        m = master_re.match(text, pos)
        if m:
            kind = m.lastgroup
            value = m.group(kind)
            if kind == "WS":
                # skip whitespace
                pass
            else:
                tokens.append((kind, value))
            pos = m.end()
        else:
            raise SyntaxError(f"Illegal character at position {pos}: {text[pos]}")
    tokens.append(("EOF","EOF"))
    return tokens
