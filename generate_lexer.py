import sys

def main():
    if len(sys.argv) < 2:
        print("Usage: python generate_lexer.py lexical_spec.txt")
        sys.exit(1)
    spec_file = sys.argv[1]

    rules = []
    with open(spec_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            parts = line.split(None, 1)
            if len(parts) == 2:
                token, pattern = parts
                pattern = pattern.strip()
                rules.append((token, pattern))

    # 不实现NFA/DFA构造，直接使用re
    with open('lexer.py', 'w', encoding='utf-8') as out:
        out.write('''# Auto-generated by generate_lexer.py
import re

token_specs = [
''')
        for token, pattern in rules:
            out.write(f'    ("{token}", r"{pattern}"),\n')
        out.write('''    
]

token_regex = "|".join(f"(?P<{t}>{p})" for t,p in token_specs)
#print("Final token regex:", token_regex) 
master_re = re.compile(token_regex, re.VERBOSE | re.UNICODE)

def tokenize(text):
    pos = 0
    tokens = []
    while pos < len(text):
        m = master_re.match(text, pos)
        if m:
            kind = m.lastgroup
            value = m.group(kind)
            if kind == "WS":
                # skip whitespace
                pass
            else:
                tokens.append((kind, value))
            pos = m.end()
        else:
            context = text[max(pos-5, 0):pos+5]
            raise SyntaxError(f"Illegal character '{text[pos]}' at position {pos}. Context: ...{context}...")
    tokens.append(("EOF", "EOF"))
    return tokens
''')

    print("lexer.py generated.")

if __name__ == '__main__':
    main()