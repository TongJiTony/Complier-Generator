# 项目文档

## 项目背景

本项目是编译原理课程的一个期末大作业，旨在完成一个简易的“编译器生成器”原型，主要涵盖了编译过程中的以下阶段：

- **词法分析（Lexical Analysis）**  
  通过正则表达式来切分和识别单词 (Token)，如 IF, ID, NUM 等。

- **语法分析（Syntax Analysis）**  
  根据无上下文文法，对 Token 序列进行语法规则匹配，构建抽象语法树 (AST)。

- **语法制导翻译（Syntax-Directed Translation） / 中间代码生成**  
  递归遍历 AST，生成三地址中间代码 (Three Address Code)，包括临时变量 `t1, t2, ...` 和标签 `L1, L2, ...` 等。

通过本项目，可以初步掌握编译器核心流程，对后续学习编译优化、目标代码生成等都有一定帮助。

---

## 总体设计

### 编译流程概览

```text
源语言源码 
   └──(lexer)──>   Token序列 
               └──(parser)──>   抽象语法树(AST) 
                            └──(代码生成器)──>   三地址中间代码
```

### 项目文件结构

```plaintext
|-- lexical_spec.txt       # 词法定义
|-- grammar_spec.txt       # 文法定义
|-- generate_lexer.py      # 自动生成 lexer.py
|-- generate_parser.py     # 自动生成 parser.py
|-- compiler.py            # 编译器主程序（生成三地址码）
|-- lexer.py               # 由 generate_lexer.py 生成
|-- parser.py              # 由 generate_parser.py 生成
...
```

---

## 系统实现

### 1. 词法分析生成器（generate_lexer.py）

#### 功能简介

- **输入：** `lexical_spec.txt`  
  包含形如 `IF if / ID [a-zA-Z_][a-zA-Z0-9_]*` 的多行，每行包含 Token 类型和正则表达式。
- **输出：** `lexer.py`  
  包含 `token_specs`, `token_regex`, `tokenize(text)` 等核心部分。

#### 关键逻辑

- **读取词法规则**

```python
with open(spec_file, 'r', encoding='utf-8') as f:
    for line in f:
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        parts = line.split(None, 1)
        if len(parts) == 2:
            token, pattern = parts
            rules.append((token, pattern))
```

- **生成正则表达式**

```python
token_regex = "|".join(f"(?P<{t}>{p})" for t, p in token_specs)
master_re = re.compile(token_regex)
```

- **匹配文本**

```python
while pos < len(text):
    m = master_re.match(text, pos)
    if m:
        kind = m.lastgroup
        value = m.group(kind)
        if kind == "WS":  # 跳过空白符
            pass
        else:
            tokens.append((kind, value))
        pos = m.end()
    else:
        raise SyntaxError("Illegal character...")
```

---

### 2. 语法分析生成器（generate_parser.py）

#### 功能简介

- **输入：** `grammar_spec.txt`  
  包含无上下文文法的产生式。
- **输出：** `parser.py`  
  包含以下内容：
  - `productions`、`start_symbol`（文法记录）
  - `RecursiveParser` 类（递归下降分析实现）
  - `Node` 类（表示 AST 节点）

#### 关键逻辑

- **读取文法文件**

```python
with open(grammar_file, 'r', encoding='utf-8') as f:
    for line in f:
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        left, right = line.split('->', 1)
        rules.append((left.strip(), right.strip().split()))
```

- **递归下降子程序**

```python
def parse_Program(self):
    node = Node("Program")
    node.children.append(self.parse_StmtList())
    return node
```

- **AST 节点结构**

```python
class Node:
    def __init__(self, type, children=None, value=None):
        self.type = type
        self.children = children or []
        self.value = value
```
### 语法分析器（parser.py）
本解析器旨在处理一种简化版编程语言的源代码，该语言包括赋值语句、条件语句（if）、循环语句（while），以及基本的算术和布尔表达式。解析器的目标是将输入的词法分析的标记流转化为抽象语法树（AST），以便后续的编译或解释过程。
### 方法详解
2.2输入输出
输入：来自词法分析器的标记流（token stream），而不是直接处理源代码。
输出：返回一个表示输入源代码的 AST 根节点对象；如果解析失败，则抛出异常并打印错误信息。
2.3详细设计
2.3.1类与函数说明
Node 类：表示 AST 中的一个节点，包含节点类型、子节点列表、值和其他相关属性。
属性：
type：节点的类型（如 "Program", "AssignStmt" 等）。
children：子节点列表。
value：节点的值（例如标识符名或数字）。
attributes：其他相关属性（如类型信息）。
函数方法：
set_attribute(key, value)：设置节点的某个属性。
get_attribute(key)：获取节点的某个属性。
_ _repr__()：返回节点的字符串表示形式，便于调试。
### 
RecursiveParser 类：实现递归下降解析算法的核心逻辑，并在解析过程中计算综合属性和传递继承属性。
设计思路：
层次化设计：解析函数按运算符优先级分层，确保复杂的表达式可以被正确解析。
递归调用：通过递归调用实现对复杂语法规则的支持，例如嵌套的表达式和多分支语句。
类型检查与符号表：在解析过程中进行类型检查和符号表管理，确保语义正确性。
模块化设计：每个函数专注于特定类型的解析任务，便于维护和扩展。
##
初始化：接收一个标记流（token stream）作为输入。
初始化解析器的位置指针 pos 和符号表 symbol_table。
##
函数方法：
at_end()：检查是否到达标记流的末尾。
lookahead()：查看当前指向的标记，但不移动指针。
match(token_type)：调用lookahead，尝试匹配特定类型的标记，成功时前进到下一个标记。
consume(token_type)：调用match，其作用是确认当前的标记是否符合预期，并在确认后将解析位置向前移动到下一个标记。如果不符合预期，则抛出异常。
##
层次化结构
第一层程序解析入口
parse_Program：返回AST的根节点
parse_StmtList()：解析语句列表直到遇到结束或者非语句标记，返回相应的 AST 节点。
                 stmtllist包含多个stmt，每个stmt对应一个语句

第二层：parse_Stmt：根据当前标记选择合适的解析方法来解析单个语句，调用第三层的三种

第三层：具体语句解析
parse_AssignStmt()：解析赋值语句，计算表达式的值，并将其作为综合属性存储在节点中。
parse_IfStmt()：解析条件语句，确保条件表达式的类型正确，并传递必要的继承属性。
parse_WhileStmt()：解析循环语句，确保条件表达式的类型正确，并传递必要的继承属性。

第四层：基本表达式解析 (算术和布尔表达式)
算术表达式：
parse_Expr：处理加法和减法运算。
parse_Term：处理乘法和除法运算。
parse_Factor：处理最基础的因子，如数字、变量或括号内的表达式。
布尔表达式：
parse_BooleanExpr：处理逻辑或运算 (||)。
parse_BooleanTerm：处理逻辑与运算 (&&)。
parse_BooleanNot：处理逻辑非运算 (!)。
parse_RelExpr：处理关系运算（如 >, < 等）

---

### 3. 编译器主程序（compiler.py）

#### 功能简介

从源程序中读取文本内容，调用 `tokenize()` 和 `parse()` 得到 Token 列表和 AST；基于 AST，生成三地址中间代码。

#### 关键逻辑

- **生成三地址码**

```python
def generate_3ac(node, code):
    if node.type == "Program":
        for child in node.children:
            generate_3ac(child, code)
    elif node.type == "AssignStmt":
        lhs = node.value
        rhs_place = generate_3ac_expr(node.children[0], code)
        code.append(f"{lhs} = {rhs_place}")
    ...
```

---

## 系统测试

### 测试环境

- **操作系统：** Windows 10 / Linux / macOS
- **Python 版本：** Python 3.7+

### 测试用例

#### 简单赋值语句

**输入：**

```c
x = 1;
y = x + 2;
```

**三地址中间代码：**

```makefile
x = 1
t1 = x + 2
y = t1
```

#### 条件分支语句

**输入：**

```c
if (x == 0) y = 1; else y = 2;
```

**三地址中间代码：**

```makefile
t1 = x == 0
if t1 == 1 goto L1
goto L2
L1:
y = 1
goto L3
L2:
y = 2
L3:
```

---

## 使用说明

1. 准备好 `lexical_spec.txt` 和 `grammar_spec.txt`。
2. 运行：

   ```bash
   python generate_lexer.py lexical_spec.txt
   python generate_parser.py grammar_spec.txt
   python compiler.py source.code
   ```

---

## 可扩展性与改进

- **文法扩展**  
  可增加函数声明、数组定义等。

- **优化与错误恢复**  
  引入短语级恢复，提高报错可读性。

- **目标代码生成**  
  后续实现汇编代码或 LLVM IR。

- **可视化**  
  添加 AST 或流程图展示，辅助调试。

---

## 总结

通过本项目，基本实现了一个简易编译器从词法分析、语法分析到三地址中间代码生成的核心流程，提供了灵活扩展的基础。
