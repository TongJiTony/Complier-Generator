# 项目文档

## 项目背景

本项目是编译原理课程的一个期末大作业，旨在完成一个简易的“编译器生成器”原型，主要涵盖了编译过程中的以下阶段：

- **词法分析（Lexical Analysis）**  
  通过正则表达式来切分和识别单词 (Token)，如 IF, ID, NUM 等。

- **语法分析（Syntax Analysis）**  
  根据无上下文文法，对 Token 序列进行语法规则匹配，构建抽象语法树 (AST)。

- **语法制导翻译（Syntax-Directed Translation） / 中间代码生成**  
  递归遍历 AST，生成三地址中间代码 (Three Address Code)，包括临时变量 `t1, t2, ...` 和标签 `L1, L2, ...` 等。

通过本项目，可以初步掌握编译器核心流程，对后续学习编译优化、目标代码生成等都有一定帮助。

---

## 总体设计

### 编译流程概览

```text
源语言源码 
   └──(lexer)──>   Token序列 
               └──(parser)──>   抽象语法树(AST) 
                            └──(代码生成器)──>   三地址中间代码
```

### 项目文件结构

```plaintext
|-- lexical_spec.txt       # 词法定义
|-- grammar_spec.txt       # 文法定义
|-- generate_lexer.py      # 自动生成 lexer.py
|-- generate_parser.py     # 自动生成 parser.py
|-- compiler.py            # 编译器主程序（生成三地址码）
|-- lexer.py               # 由 generate_lexer.py 生成
|-- parser.py              # 由 generate_parser.py 生成
...
```

---

## 系统实现

### 词法分析生成器详解

#### **1. 概述**

**词法分析器**（Lexical Analyzer, 又称 Scanner 或 Tokenizer）的主要功能是将源代码拆分成一系列符合词法规则的“记号”（Token），供语法分析器进一步处理。在传统编译器构造中，通常需要手动或借助工具（如 Lex/Flex）来编写词法分析器。本项目的 **词法分析生成器**（`generate_lexer.py`）则通过读取一个词法规格文件（`lexical_spec.txt`）并使用 Python 的正则表达式，自动生成能够执行词法分析功能的 Python 脚本 `lexer.py`。

**该自动化脚本降低了实现成本和出错率：**  
- **无需手动编写词法分析器**。  
- **更改或扩展词法规则**时只需更新 `lexical_spec.txt` 并再次运行脚本，即可生成新的词法分析器。

---

#### **2. 功能**

- **读取词法规格文件**  
  - 文件格式：每行包含两个部分：  
    - Token 名称（如 `IF`, `ID`, `NUM`, `WS` 等）  
    - 正则表达式（如 `if`, `[a-zA-Z_][a-zA-Z0-9_]*`, `[0-9]+` 等）  
  - 允许在词法规格文件中插入注释行（以 `#` 开头），以及空行。

- **自动生成 `lexer.py`**  
  - 生成的 `lexer.py` 中包含：  
    - `token_specs`: 存储所有 `(token_name, pattern)` 二元组的列表。  
    - `tokenize(text)`: 核心函数，用于对输入的源代码字符串进行正则匹配，输出 Token 列表。  
    - 其他辅助数据结构，如 `master_re`（一个合并所有 Token 正则表达式的大正则对象）。

- **提供对常见 Token 的识别能力**  
  - 如 **标识符**(`ID`)、**数字**(`NUM`)、**关键字**(`IF`, `ELSE` 等)、**运算符**(`+`, `-`, `*`, `/` 等)、**分隔符**(`(`, `)`, `;`)、以及 **空白符**(`WS`) 等。
  - 当无法匹配到任何 Token 时，会抛出 `SyntaxError`，便于在编译早期定位非法字符或拼写错误。

---

#### **3. 设计与实现思路**

- **正则表达式驱动**  
  - Python 的 `re` 模块为**编译原理**课中“由正则到 NFA/DFA”的过程提供了一条捷径。  
  - 在传统做法中，需要手写或自动生成一个有限自动机 (FA)，而在本项目中，直接使用正则库即可完成模式匹配。

- **可配置的词法规则**  
  - 将所有 Token 类型与其对应的正则模式放在 `lexical_spec.txt` 文件中，方便修改和维护。  
  - 脚本 `generate_lexer.py` 只需知道如何读取与解析这些规则，然后再把它们写到 `lexer.py` 的 `token_specs` 里。

- **命名分组**  
  - 通过 `(?P<TokenName>pattern)` 这一 Python 正则特性，可以在一次匹配过程中识别不同类型的 Token。  
  - 这样，**一个大的正则**就可以捕获多种 Token 类型，而无需在循环中一个一个尝试。

- **自动跳过空白**  
  - 通常我们都会有一个 `WS`（whitespace）Token 对应 `[ \t\n]+` 等正则，如果匹配到它，就**不放进 Token 列表**，从而实现**自动忽略空格和换行**。

- **增量式匹配**  
  - 在 `tokenize()` 函数中，通过一个 `pos` 指针，从文本开头一路匹配到结尾：  
    - 如果当前位置 `pos` 匹配成功，就将其加入 Token 列表（或跳过空白）。  
    - 如果匹配失败，抛出异常。

---

#### **4. 关键算法与逻辑**

**4.1 读取并解析词法规则**  

```python
with open(spec_file, 'r', encoding='utf-8') as f:
    for line in f:
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        parts = line.split(None, 1)
        if len(parts) == 2:
            token, pattern = parts
            rules.append((token, pattern))
```

- **操作**：逐行读取 `lexical_spec.txt`，忽略空行和注释行。  
- **要点**：使用 `line.split(None, 1)`，只切分第一次空白分隔，以得到“标记类型”和“正则表达式”两部分。

---

**4.2 拼接大正则表达式**

```python
with open('lexer.py', 'w', encoding='utf-8') as out:
    out.write('''# Auto-generated by generate_lexer.py
import re

token_specs = [
''')
    for token, pattern in rules:
        out.write(f'    ("{token}", r"{pattern}"),\n')
    out.write(''']
token_regex = "|".join(f"(?P<{t}>{p})" for t, p in token_specs)
master_re = re.compile(token_regex)
...
''')
```

- **原理**：将每个 Token 的正则模式写成 **命名分组** 的形式，例如 `(?P<IF>if)`、`(?P<NUM>[0-9]+)`，再用 `|` 管道运算符连接起来。
- **示例**：若有三条规则 `("IF", "if")`, `("ID", "[a-zA-Z_][a-zA-Z0-9_]*")`, `("NUM", "[0-9]+")`，则生成的大正则类似：
  ```
  (?P<IF>if)|(?P<ID>[a-zA-Z_][a-zA-Z0-9_]*)|(?P<NUM>[0-9]+)
  ```
- **优点**：只需一次 `master_re.match()` 就能获知匹配到的是哪一种 Token。

---

**4.3 核心匹配循环：`tokenize(text)`**

```python
def tokenize(text):
    pos = 0
    tokens = []
    while pos < len(text):
        m = master_re.match(text, pos)
        if m:
            kind = m.lastgroup
            value = m.group(kind)
            if kind == "WS":
                # skip whitespace
                pass
            else:
                tokens.append((kind, value))
            pos = m.end()
        else:
            raise SyntaxError(f"Illegal character at position {pos}: {text[pos]}")
    tokens.append(("EOF", "EOF"))
    return tokens
```

---

#### **5. 使用示例**

1. **词法规格示例**（`lexical_spec.txt`）：

```plaintext
IF      if
ELSE    else
WHILE   while
ID      [a-zA-Z_][a-zA-Z0-9_]*
NUM     [0-9]+
ASSIGN  =
PLUS    \+
MINUS   -
MUL     \*
DIV     /
LPAREN  \(
RPAREN  \)
SEMI    ;
RELOP   (==|!=|<=|>=|<|>)
AND     &&
OR      \|\|
NOT     !
WS      [ \t\n]+
```

2. **生成 `lexer.py`**

```bash
python generate_lexer.py lexical_spec.txt
```

3. **调用 `lexer.py` 中的 `tokenize()`**

```python
from lexer import tokenize
source_code = "if (x == 0) x = x + 1;"
tokens = tokenize(source_code)
print(tokens)

---

### 2. 语法分析生成器（generate_parser.py）

#### 功能简介

- **输入：** `grammar_spec.txt`  
  包含无上下文文法的产生式。
- **输出：** `parser.py`  
  包含以下内容：
  - `productions`、`start_symbol`（文法记录）
  - `RecursiveParser` 类（递归下降分析实现）
  - `Node` 类（表示 AST 节点）

#### 关键逻辑

- **读取文法文件**

```python
with open(grammar_file, 'r', encoding='utf-8') as f:
    for line in f:
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        left, right = line.split('->', 1)
        rules.append((left.strip(), right.strip().split()))
```

- **递归下降子程序**

```python
def parse_Program(self):
    node = Node("Program")
    node.children.append(self.parse_StmtList())
    return node
```

- **AST 节点结构**

```python
class Node:
    def __init__(self, type, children=None, value=None):
        self.type = type
        self.children = children or []
        self.value = value
```
### 语法分析器（parser.py）
本解析器旨在处理一种简化版编程语言的源代码，该语言包括赋值语句、条件语句（if）、循环语句（while），以及基本的算术和布尔表达式。解析器的目标是将输入的词法分析的标记流转化为抽象语法树（AST），以便后续的编译或解释过程。
### 方法详解
2.2输入输出
输入：来自词法分析器的标记流（token stream），而不是直接处理源代码。
输出：返回一个表示输入源代码的 AST 根节点对象；如果解析失败，则抛出异常并打印错误信息。
2.3详细设计
2.3.1类与函数说明
Node 类：表示 AST 中的一个节点，包含节点类型、子节点列表、值和其他相关属性。
属性：
type：节点的类型（如 "Program", "AssignStmt" 等）。
children：子节点列表。
value：节点的值（例如标识符名或数字）。
attributes：其他相关属性（如类型信息）。
函数方法：
set_attribute(key, value)：设置节点的某个属性。
get_attribute(key)：获取节点的某个属性。
_ _repr__()：返回节点的字符串表示形式，便于调试。
### 
RecursiveParser 类：实现递归下降解析算法的核心逻辑，并在解析过程中计算综合属性和传递继承属性。
设计思路：
层次化设计：解析函数按运算符优先级分层，确保复杂的表达式可以被正确解析。
递归调用：通过递归调用实现对复杂语法规则的支持，例如嵌套的表达式和多分支语句。
类型检查与符号表：在解析过程中进行类型检查和符号表管理，确保语义正确性。
模块化设计：每个函数专注于特定类型的解析任务，便于维护和扩展。
##
初始化：接收一个标记流（token stream）作为输入。
初始化解析器的位置指针 pos 和符号表 symbol_table。
##
函数方法：
at_end()：检查是否到达标记流的末尾。
lookahead()：查看当前指向的标记，但不移动指针。
match(token_type)：调用lookahead，尝试匹配特定类型的标记，成功时前进到下一个标记。
consume(token_type)：调用match，其作用是确认当前的标记是否符合预期，并在确认后将解析位置向前移动到下一个标记。如果不符合预期，则抛出异常。
##
层次化结构
第一层程序解析入口
parse_Program：返回AST的根节点
parse_StmtList()：解析语句列表直到遇到结束或者非语句标记，返回相应的 AST 节点。
                 stmtllist包含多个stmt，每个stmt对应一个语句

第二层：parse_Stmt：根据当前标记选择合适的解析方法来解析单个语句，调用第三层的三种

第三层：具体语句解析
parse_AssignStmt()：解析赋值语句，计算表达式的值，并将其作为综合属性存储在节点中。
parse_IfStmt()：解析条件语句，确保条件表达式的类型正确，并传递必要的继承属性。
parse_WhileStmt()：解析循环语句，确保条件表达式的类型正确，并传递必要的继承属性。

第四层：基本表达式解析 (算术和布尔表达式)
算术表达式：
parse_Expr：处理加法和减法运算。
parse_Term：处理乘法和除法运算。
parse_Factor：处理最基础的因子，如数字、变量或括号内的表达式。
布尔表达式：
parse_BooleanExpr：处理逻辑或运算 (||)。
parse_BooleanTerm：处理逻辑与运算 (&&)。
parse_BooleanNot：处理逻辑非运算 (!)。
parse_RelExpr：处理关系运算（如 >, < 等）

---

### 3. 编译器主程序（compiler.py）

#### 功能简介

从源程序中读取文本内容，调用 `tokenize()` 和 `parse()` 得到 Token 列表和 AST；基于 AST，生成三地址中间代码。

#### 关键逻辑

- **生成三地址码**

```python
def generate_3ac(node, code):
    if node.type == "Program":
        for child in node.children:
            generate_3ac(child, code)
    elif node.type == "AssignStmt":
        lhs = node.value
        rhs_place = generate_3ac_expr(node.children[0], code)
        code.append(f"{lhs} = {rhs_place}")
    ...
```

---

## 系统测试

### 测试环境

- **操作系统：** Windows 10 / Linux / macOS
- **Python 版本：** Python 3.7+

### 测试用例

#### 简单赋值语句

**输入：**

```c
x = 1;
y = x + 2;
```

**三地址中间代码：**

```makefile
x = 1
t1 = x + 2
y = t1
```

#### 条件分支语句

**输入：**

```c
if (x == 0) y = 1; else y = 2;
```

**三地址中间代码：**

```makefile
t1 = x == 0
if t1 == 1 goto L1
goto L2
L1:
y = 1
goto L3
L2:
y = 2
L3:
```

---

## 使用说明

1. 准备好 `lexical_spec.txt` 和 `grammar_spec.txt`。
2. 运行：

   ```bash
   python generate_lexer.py lexical_spec.txt
   python generate_parser.py grammar_spec.txt
   python compiler.py source.code
   ```

---

## 可扩展性与改进

- **文法扩展**  
  可增加函数声明、数组定义等。

- **优化与错误恢复**  
  引入短语级恢复，提高报错可读性。

- **目标代码生成**  
  后续实现汇编代码或 LLVM IR。

- **可视化**  
  添加 AST 或流程图展示，辅助调试。

---

## 总结

通过本项目，基本实现了一个简易编译器从词法分析、语法分析到三地址中间代码生成的核心流程，提供了灵活扩展的基础。
