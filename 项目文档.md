# 项目文档

## 项目背景

本项目是编译原理课程的一个期末大作业，旨在完成一个简易的“编译器生成器”原型，主要涵盖了编译过程中的以下阶段：

- **词法分析（Lexical Analysis）**  
  通过正则表达式来切分和识别单词 (Token)，如 IF, ID, NUM 等。

- **语法分析（Syntax Analysis）**  
  根据无上下文文法，对 Token 序列进行语法规则匹配，构建抽象语法树 (AST)。

- **语法制导翻译（Syntax-Directed Translation） / 中间代码生成**  
  递归遍历 AST，生成三地址中间代码 (Three Address Code)，包括临时变量 `t1, t2, ...` 和标签 `L1, L2, ...` 等。

通过本项目，可以初步掌握编译器核心流程，对后续学习编译优化、目标代码生成等都有一定帮助。

---

## 总体设计

### 编译流程概览

```text
源语言源码 
   └──(lexer)──>   Token序列 
               └──(parser)──>   抽象语法树(AST) 
                            └──(代码生成器)──>   三地址中间代码
```

### 项目文件结构

```plaintext
|-- lexical_spec.txt       # 词法定义
|-- grammar_spec.txt       # 文法定义
|-- generate_lexer.py      # 自动生成 lexer.py
|-- generate_parser.py     # 自动生成 parser.py
|-- compiler.py            # 编译器主程序（生成三地址码）
|-- lexer.py               # 由 generate_lexer.py 生成
|-- parser.py              # 由 generate_parser.py 生成
...
```

---

## 系统实现

### 1. 词法分析生成器（generate_lexer.py）

#### 功能简介

- **输入：** `lexical_spec.txt`  
  包含形如 `IF if / ID [a-zA-Z_][a-zA-Z0-9_]*` 的多行，每行包含 Token 类型和正则表达式。
- **输出：** `lexer.py`  
  包含 `token_specs`, `token_regex`, `tokenize(text)` 等核心部分。

#### 关键逻辑

- **读取词法规则**

```python
with open(spec_file, 'r', encoding='utf-8') as f:
    for line in f:
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        parts = line.split(None, 1)
        if len(parts) == 2:
            token, pattern = parts
            rules.append((token, pattern))
```

- **生成正则表达式**

```python
token_regex = "|".join(f"(?P<{t}>{p})" for t, p in token_specs)
master_re = re.compile(token_regex)
```

- **匹配文本**

```python
while pos < len(text):
    m = master_re.match(text, pos)
    if m:
        kind = m.lastgroup
        value = m.group(kind)
        if kind == "WS":  # 跳过空白符
            pass
        else:
            tokens.append((kind, value))
        pos = m.end()
    else:
        raise SyntaxError("Illegal character...")
```

---

### 2. 语法分析生成器（generate_parser.py）

#### 功能简介

- **输入：** `grammar_spec.txt`  
  包含无上下文文法的产生式。
- **输出：** `parser.py`  
  包含以下内容：
  - `productions`、`start_symbol`（文法记录）
  - `RecursiveParser` 类（递归下降分析实现）
  - `Node` 类（表示 AST 节点）

#### 关键逻辑

- **读取文法文件**

```python
with open(grammar_file, 'r', encoding='utf-8') as f:
    for line in f:
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        left, right = line.split('->', 1)
        rules.append((left.strip(), right.strip().split()))
```

- **递归下降子程序**

```python
def parse_Program(self):
    node = Node("Program")
    node.children.append(self.parse_StmtList())
    return node
```

- **AST 节点结构**

```python
class Node:
    def __init__(self, type, children=None, value=None):
        self.type = type
        self.children = children or []
        self.value = value
```

---

### 3. 编译器主程序（compiler.py）

#### 功能简介

从源程序中读取文本内容，调用 `tokenize()` 和 `parse()` 得到 Token 列表和 AST；基于 AST，生成三地址中间代码。

#### 关键逻辑

- **生成三地址码**

```python
def generate_3ac(node, code):
    if node.type == "Program":
        for child in node.children:
            generate_3ac(child, code)
    elif node.type == "AssignStmt":
        lhs = node.value
        rhs_place = generate_3ac_expr(node.children[0], code)
        code.append(f"{lhs} = {rhs_place}")
    ...
```

---

## 系统测试

### 测试环境

- **操作系统：** Windows 10 / Linux / macOS
- **Python 版本：** Python 3.7+

### 测试用例

#### 简单赋值语句

**输入：**

```c
x = 1;
y = x + 2;
```

**三地址中间代码：**

```makefile
x = 1
t1 = x + 2
y = t1
```

#### 条件分支语句

**输入：**

```c
if (x == 0) y = 1; else y = 2;
```

**三地址中间代码：**

```makefile
t1 = x == 0
if t1 == 1 goto L1
goto L2
L1:
y = 1
goto L3
L2:
y = 2
L3:
```

---

## 使用说明

1. 准备好 `lexical_spec.txt` 和 `grammar_spec.txt`。
2. 运行：

   ```bash
   python generate_lexer.py lexical_spec.txt
   python generate_parser.py grammar_spec.txt
   python compiler.py source.code
   ```

---

## 可扩展性与改进

- **文法扩展**  
  可增加函数声明、数组定义等。

- **优化与错误恢复**  
  引入短语级恢复，提高报错可读性。

- **目标代码生成**  
  后续实现汇编代码或 LLVM IR。

- **可视化**  
  添加 AST 或流程图展示，辅助调试。

---

## 总结

通过本项目，基本实现了一个简易编译器从词法分析、语法分析到三地址中间代码生成的核心流程，提供了灵活扩展的基础。
